---
title: 2025名古屋機「シロ」ソフトウェア解説②レスキュー編 #タイトル
image: /img/2025/0507/hero.png
author: mitu #名前　shuji,kapi,mituから選ぶ
tags: #箇条書きでタグをつける
  - ソフトウェア
---

レスキュー部のソフトウェアを担当していたmituです。レスキュー時のシロの制御はshujiに任せていたので(ごめんなさい)被災者と避難場所検出の話をします。

2025全国大会の機体「シロ」のレスキュー部のソフトウェアの解説を行います。

# はじめに
ハードウェアについては、[ハードウェア解説](https://tuton-rcj.jp/20250405/)及び[基板解説](https://tuton-rcj.jp/20250427/)、ライントレースのソフトウェア部については[ソフトウェア解説①ライントレース編](https://tuton-rcj.jp/20250507)をご覧ください。


OpenMVはOpenMV IDEにてMicroPythonで、その他のマイコンはPlatformIOにてArduino言語（C/C++）で開発しています。


私たちのソフトウェアは以下の通りGitHubで公開しています。


東東京ノード：[https://github.com/tuton-RCJ/Tokyo2025Software](https://github.com/tuton-RCJ/Tokyo2025Software)


関東ブロック：[https://github.com/tuton-RCJ/Kanto2025Software](https://github.com/tuton-RCJ/Kanto2025Software)


全国：[https://github.com/tuton-RCJ/NagoyaSoftware](https://github.com/tuton-RCJ/NagoyaSoftware)



# 被災者、避難場所検出
被災者、避難場所検出にはOpenMV H7 R1を用いています。

該当コードは[こちら](https://github.com/tuton-RCJ/NagoyaSoftware/blob/main/OpenMV/main.py)

## 全体の話
検出結果をそのまま返すのではなく、前10回分の検出結果を記憶しておき、それらに対してクラスタリングを行うことで多少のノイズ、誤検出が生じたとしても動作が安定するようになりました。


また、座標をタプル`(x,y)`として持つのではなく整数型`(x<<8)+y`として持つ事で2倍程度の高速化に成功しました。


## 避難場所検出
赤と緑の避難場所は組み込み関数の`image.find_blobs`を用いて検出しています。


閾値を使った古典的な方法ながら、避難場所の色が単色であったこともあってかなり精度が出ましたが、OpenMVデフォルトの露光時間では黒との区別がつかなかったため、露光時間を4倍から5倍程度に上げています。


また、大会では立体交差を支える柱を赤の避難場所と誤検出してしまうことがいくつかあり、応急処置として縦横比などを用いてある程度外形に対して制限をかけました。(本競技ではそもそも黒被災者を拾うことができませんでしたが…)


### 既知の不具合
いくつかのイメージセンサでは露光時間やホワイトバランスが手動で操作できなくなることがあります。大会3日目でこれが発生して困りました。


## 黒被災者検出
黒被災者は候補となる円を`image.find_circles`によって挙げたのちに、その領域の明度を使って絞り込み、検出しています。


候補を見つける際、避難場所の検出と同様に`image.find_blobs`等を用いる案もありましたが最終的にはこちらの方になりました。特に、出口付近にいる黒被災者に対して強く出ることができます。


また、黒と緑を見分けられるように画像に対してヒストグラム均一化を行っています。


## 銀被災者検出
[Edge Impulse](https://edgeimpulse.com/)というプラットフォーム上で事前に用意した500枚程度の銀被災者を含む救助ゾーンの写真に対してFOMOから転移学習を行い、そのモデルを用いて検出しています。


関東では黒、銀ともに学習させていたのに対して、前述の通り黒被災者をアルゴリズムベースで検出するようにしたことで、銀被災者の検出精度が大幅に向上しました。


また、黒被災者でも行ったことなのですが、被災者を拾った際にアームの中に実際に被災者がいるか確認することで、拾えなかったのに被災者を避難場所に運ぼうとしてしまうのを防止しています。


# 質問コーナー
いただいた質問にお答えします。


Q.関東ではEdge impluseだったと思いますが、画像検出のツールは何を使いましたか？

A.前述の通り、銀被災者は関東と同じくEdge Impulseを使い、黒被災者はアルゴリズムベースで検出しました。


Q.全国大会の救出コーナーの壁には銀色の支柱が使われていたと思いますが、これを生存者としてカメラが検出してしまうような誤作動は起きませんでしたか？また、これに対しなにか対策をしましたか？

A.銀色の支柱を被災者と誤検出するようなことは無かったです、が、試走コースの救助ゾーンに設置されていたバンプを何回か誤検出していました。機械学習って不思議。


Q.走行テストはどのような環境を作ってやっていましたか？また何か工夫をしましたか？

A.ライントレースでは部活動で受け継がてきたタイルを用いてコースを作成し、試走を行っていました。また救助ゾーンは横一タイル分の壁用の板と床用の板、および避難場所用の板を用いて作成しました。こうすることによって、大会現地でも壁用の板と避難場所用の板さえあれば簡易的に救助ゾーンを組むことができ、現地での調整に役立ちました。


# おわりに


今回初めてカメラを使って被災者救助を行いました。カメラを使った被災者救助はかなり直感的でToF等を用いた検出より高速にできて楽しかったのですが、一方でマイコンの性能限界を感じる場面が多かったです。


特に、500枚程度の画像で学習したモデルで銀被災者と黒被災者を世界大会の救助ゾーンのような障害物や立体交差下で誤検出無く発見することは相当に厳しいと感じています。実際、2024世界大会優勝のOverEngineeringはRaspberry Pi 5 8GBを積み、3145枚の画像で学習させています。また、AIに頼らないアルゴベースの検出手段として[FRST](https://link.springer.com/content/pdf/10.1007/3-540-47969-4_24.pdf)などがありますが、これもリアルタイムに行おうとするとかなりの性能が必要です。


日本でもこのような手法を上手く活かしたチームが増えていくといいなと思います。

関東ブロック大会に出場していたテクノトルクス（[https://techno-robocup.github.io/](https://techno-robocup.github.io/)）はラズパイを乗せて高精度の被災者検出を実現していました。来年も出場するようなので頑張ってほしいなと思います。